# -*- coding: utf-8 -*-
"""AnistyaRosyida_KALBE_DS_MachineLearningRegression&Clustering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VZmyr8j8Qg-y3Qgrv9mvXsQoM78CTpTa
"""

pip install statsmodels

"""Pustaka statsmodels adalah pustaka yang digunakan untuk analisis statistik dan estimasi model statistik dalam Python. Ini mencakup berbagai jenis model statistik, termasuk model deret waktu, regresi, analisis varians, dan banyak lagi."""

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

from sklearn.preprocessing import LabelEncoder

from sklearn import preprocessing

from statsmodels.tsa.seasonal import seasonal_decompose
from pandas.plotting import autocorrelation_plot
from statsmodels.tsa.arima_model import ARIMA
from statsmodels.tsa.arima.model import ARIMA as sm_ARIMA

from sklearn.metrics import mean_absolute_error, mean_squared_error

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler

df_Customer = pd.read_csv("/content/Customer.csv")
df_Customer.head()

df_Customer.info()

#encode otomatis
#pembuatan instance labelcoder
le = LabelEncoder()

cols_dua_unique = ['Marital Status']

for col in cols_dua_unique:
  df_Customer[col] = le.fit_transform(df_Customer[col])

df_Customer.head()

df_Customer.info()

df_Customer.isnull().sum()

df_Customer.duplicated().sum()

df_Product = pd.read_csv("/content/Product.csv")
df_Product.head()

df_Product.info()

df_Product.isnull().sum()

df_Product.duplicated().sum()

df_Store = pd.read_csv("/content/Store.csv")
df_Store.head()

df_Store.info()

#encode otomatis
#pembuatan instance labelcoder
le = LabelEncoder()

cols_dua_unique = ['GroupStore', 'Type']

for col in cols_dua_unique:
  df_Store[col] = le.fit_transform(df_Store[col])

df_Store.head()

df_Store.info()

df_Store.isnull().sum()

df_Store.duplicated().sum()

df_Transaction = pd.read_csv("/content/Transaction.csv")
df_Transaction.head()

df_Transaction.info()

df_Transaction = df_Transaction.astype({'Date' : 'datetime64[ns]'})

df_Transaction.info()

df_Transaction.isnull().sum()

df_Transaction.duplicated().sum()

# Menggabungkan data pertama dan kedua
df_merge = pd.merge(df_Customer, df_Transaction, on='CustomerID')

# Menggabungkan data ketiga dengan data yang telah digabungkan sebelumnya
df_merge = pd.merge(df_merge, df_Product, on='ProductID')

# Menggabungkan data keempat dengan data yang telah digabungkan sebelumnya
df_merge = pd.merge(df_merge, df_Store, on='StoreID')

# Menyimpan data gabungan ke dalam file CSV baru
df_merge.to_csv('Data_Merge.csv', index=False)

"""Menyimpan Data ke dalam File CSV:
Setelah data digabungkan menjadi satu DataFrame (Data_Merge), kita menggunakan metode .to_csv() untuk menyimpan data tersebut ke dalam file baru dengan nama Data_Merge.csv. Opsi index=False digunakan agar indeks DataFrame tidak disimpan sebagai kolom tambahan dalam file CSV.
"""

df_merge.head()

df_merge.info()

df_merge.isnull().sum()

df_merge.duplicated().sum()

"""# Machine Learning Regression (Time Series)"""

# Membaca data dari file CSV ke dalam DataFrame
data = pd.read_csv('/content/Data_Merge.csv')

# Operasi groupby dan agregasi
aggregated_data = data.groupby('Date')['Qty'].sum().reset_index()

# Mencetak data baru yang dihasilkan
print(aggregated_data)

aggregated_data.info()

aggregated_data['Date'] = pd.to_datetime(aggregated_data['Date'])

aggregated_data.info()

aggregated_data.isnull().sum()

aggregated_data.duplicated().sum()

"""Kode di bawah ini bertujuan untuk melakukan analisis dekomposisi musiman (seasonal decomposition) pada data deret waktu dan menampilkan hasil dekomposisinya dalam tiga grafik berbeda untuk tren, musiman, dan residu. Kode ini menggambarkan tren, musiman, dan komponen residu dari data deret waktu menggunakan pustaka statsmodels dan matplotlib"""

decomposed = seasonal_decompose(aggregated_data.set_index('Date'))

plt.figure(figsize=(8, 8))

plt.subplot(311)
decomposed.trend.plot(ax=plt.gca())
plt.title('Trend')
plt.subplot(312)
decomposed.seasonal.plot(ax=plt.gca())
plt.title('Seasonality')
plt.subplot(313)
decomposed.resid.plot(ax=plt.gca())
plt.title('Residuals')

plt.tight_layout()

"""Tren:

Tren merujuk pada perubahan jangka panjang dalam data seiring waktu.
Ini adalah komponen yang menggambarkan arah umum atau kecenderungan dalam data.
Tren bisa mengungkapkan perubahan yang terjadi dalam jangka panjang, seperti pertumbuhan ekonomi tahunan atau penurunan seiring waktu.


Seasonality:

Seasonality adalah fluktuasi yang terjadi dalam pola berulang pada interval waktu tertentu, seperti harian, bulanan, atau tahunan.
Ini mengacu pada pola yang terjadi secara konsisten pada titik-titik waktu tertentu dalam satu siklus waktu.


Residu:

Residu adalah perbedaan antara data aktual dan prediksi yang dihasilkan oleh tren dan komponen musiman.
Ini mencerminkan fluktuasi acak atau variabilitas yang tidak dapat dijelaskan oleh tren atau seasonality.
Residu yang besar menunjukkan bahwa ada variasi yang tidak dapat dijelaskan oleh tren atau seasonality dalam model dekomposisi.
"""

cut_off = round(aggregated_data.shape[0] * 0.8)
df_train = aggregated_data[:cut_off]
df_test = aggregated_data[cut_off:].reset_index(drop=True)
df_train.shape, df_test.shape

df_train

df_test

plt.figure(figsize=(20,5))
sns.lineplot(data=df_train, x=df_train['Date'], y=df_train['Qty']);
sns.lineplot(data=df_test, x=df_test['Date'], y=df_test['Qty']);

autocorrelation_plot(aggregated_data['Qty']);

"""Plot ini akan menunjukkan bagaimana nilai-nilai kuantitas pada waktu yang berbeda berkorelasi dengan nilai-nilai pada waktu sekarang, serta korelasi pada interval waktu yang lebih jauh. Informasi ini dapat membantu Anda memahami struktur dan pola dari deret waktu yang Anda analisis."""

# Assuming you have already loaded your DataFrame df_train and df_test

df_train = df_train.set_index('Date')
df_test = df_test.set_index('Date')

y = df_train['Qty']

# Fit ARIMA model
arima_order = (40, 2, 1)
ARIMAmodel = sm_ARIMA(y, order=arima_order)
ARIMAmodel = ARIMAmodel.fit()

# Forecast
forecast_steps = len(df_test)
y_pred_forecast = ARIMAmodel.get_forecast(steps=forecast_steps).predicted_mean

# Plotting
plt.figure(figsize=(20, 5))
plt.plot(df_train['Qty'], label='Training Data')
plt.plot(df_test['Qty'], color='red', label='Test Data')
plt.plot(y_pred_forecast, color='black', label='ARIMA Forecast')
plt.legend()
plt.show()

"""**Business Decisions Analysis:** Dari hasil prediksi yang didapatkan, dapat disimpulkan bahwa perkiraan quantity produk yang terjual adalah cenderung menurun dan tidak terlalu banyak sehingga tim inventory dapat menyesuaikan stok persediaan produk harian dengan menyediakan produk secukupnya agar produk tidak tersisa terlalu banyak.

Grafik ini memberikan gambaran tentang bagaimana model ARIMA yang telah dilatih dengan data pelatihan mampu memprediksi data uji yang belum dilihat sebelumnya. Anda dapat melihat sejauh mana prediksi model mendekati data uji yang sebenarnya. Jika prediksi model berdekatan dengan data uji, ini menunjukkan bahwa model ARIMA memiliki performa yang baik dalam meramalkan data deret waktu.
"""

mae = mean_absolute_error(df_test, y_pred_forecast)
mse = mean_squared_error(df_test, y_pred_forecast)
rmse = np.sqrt(mse)

print("Mean Absolute Error:", mae)
print("Mean Squared Error:", mse)
print("Root Mean Squared Error:", rmse)

"""Mean Absolute Error (MAE): MAE mengukur rata-rata selisih absolut antara nilai prediksi dan nilai aktual. Semakin rendah MAE, semakin baik model Anda dalam membuat prediksi yang mendekati nilai aktual. Dalam konteks tertentu, angka MAE 12.023 mungkin dianggap baik, tergantung pada jenis data yang Anda analisis.

Mean Squared Error (MSE): MSE mengukur rata-rata dari kuadrat selisih antara nilai prediksi dan nilai aktual. MSE akan cenderung memberi penalti lebih besar pada perbedaan yang besar antara prediksi dan nilai aktual. Angka MSE 214.463 juga mungkin baik tergantung pada skala data.

Root Mean Squared Error (RMSE): RMSE adalah akar kuadrat dari MSE. Ini memberi bobot lebih pada kesalahan besar. Angka RMSE 14.645 juga harus dilihat dalam konteks skala data.

# Machine Learning Clustering
"""

df_merge.head()

df_merge.corr()

df_cluster = df_merge.groupby('CustomerID').agg({
    'TransactionID' : 'count',
    'Qty' : 'sum',
    'TotalAmount' : 'sum'
}).reset_index()

df_cluster.head()

df_cluster

data_cluster = df_cluster.drop(columns=['CustomerID'])

data_cluster_normalize = preprocessing.normalize(data_cluster)

"""Normalisasi dapat membantu dalam memastikan bahwa semua fitur memiliki dampak yang setara pada hasil klustering, terlepas dari perbedaan dalam skala dan distribusi."""

data_cluster_normalize

K = range(2, 8)
fits = []
score = []

for k in K:
    model = KMeans(n_clusters=k, random_state=0, n_init='auto').fit(data_cluster_normalize)
    fits.append(model)

    model_labels = model.labels_  # Get the cluster labels from the model
    score.append(silhouette_score(data_cluster_normalize, model_labels, metric='euclidean'))

"""Kode yang Anda berikan adalah contoh implementasi metode "Elbow Method" untuk menentukan jumlah kluster yang optimal dalam algoritma KMeans clustering. Metode ini melibatkan iterasi melalui berbagai jumlah kluster, membangun model KMeans pada setiap iterasi, dan menghitung skor siluet untuk setiap hasil klustering"""

kmeans = KMeans(n_clusters=k, n_init='auto')
cluster_labels = kmeans.fit_predict(data_cluster_normalize)

# Calculate silhouette score
silhouette_avg = silhouette_score(data_cluster_normalize, cluster_labels)
print("Silhouette Score:", silhouette_avg)

"""Penggunaan KMeans clustering dan perhitungan skor siluet (silhouette score) untuk mengevaluasi hasil klustering.

Skor siluet (Silhouette Score) dari 0.3518 menunjukkan nilai yang sedang di antara rentang -1 hingga 1

Jika skor siluet mendekati 1, ini menunjukkan bahwa kelompok kluster sangat baik terpisah satu sama lain, dan setiap sampel berada dalam kelompok yang sesuai dengan baik.
"""

sns.lineplot(x = K, y = score);

df_cluster['cluster_label'] = fits[2].labels_

"""digunakan untuk menambahkan kolom 'cluster_label' ke dalam DataFrame df_cluster dan mengisi kolom tersebut dengan label kluster dari model KMeans yang terletak pada indeks 2 dari daftar fits."""

df_cluster.groupby(['cluster_label']).agg({
    'CustomerID' : 'count',
    'TransactionID' : 'mean',
    'Qty' : 'mean',
    'TotalAmount' : 'mean',
})

"""Cluster 0 : Qty ke-3, TotalAmount tertinggi

Cluster 1 : Qty dan TotalAmount terendah

Cluster 2 : Qty tertinggi, TotalAmount ke-2

Cluster 3 : Qty ke-2, Total Amount ke-3

**Business Decisions Analysis: **

Cluster 0 : Memberi promo agar konsumen membeli produk dalam jumlah besar

Cluster 1 : Menggencarkan promosi agar banyak konsumen membeli produk dengan jumlah besar dan harga yang mahal

Cluster 2 :  Memberi promo pada produk-produk mahal

Cluster 3 : Menggencarkan promosi agar konsumen tertarik membeli banyak produk dengan harga mahal
"""

# Select two features for visualization (change these to the desired features)
x_feature = 'Qty'
y_feature = 'TotalAmount'

# Scatter plot
plt.figure(figsize=(10, 6))
for cluster_label, cluster_data in df_cluster.groupby('cluster_label'):
    plt.scatter(cluster_data[x_feature], cluster_data[y_feature], label=f'Cluster {cluster_label}')

plt.title('Clustering Visualization')
plt.xlabel(x_feature)
plt.ylabel(y_feature)
plt.legend()
plt.grid(True)
plt.show()

"""Hasilnya akan berupa scatter plot di mana setiap titik mewakili sampel data dan memiliki warna yang berbeda sesuai dengan kluster yang telah ditetapkan. Plot ini membantu Anda melihat pola kelompok kluster yang dihasilkan oleh algoritma KMeans."""